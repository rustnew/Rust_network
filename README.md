# ğŸ§  RÃ©seau de Neurones Profond en Rust - PrÃ©diction FinanciÃ¨re

## ğŸ“‹ Table des MatiÃ¨res
- [Architecture du RÃ©seau](#architecture-du-rÃ©seau)
- [Fonctions d'Activation](#fonctions-dactivation)
- [Propagation Avant](#propagation-avant)
- [RÃ©tropropagation](#rÃ©tropropagation)
- [EntraÃ®nement sur DonnÃ©es Bancaires](#entraÃ®nement-sur-donnÃ©es-bancaires)
- [Performance et RÃ©sultats](#performance-et-rÃ©sultats)
- [Avantages de Rust](#avantages-de-rust)

---

## ğŸ—ï¸ Architecture du RÃ©seau

### Structure en Couches
```rust
pub struct DeepNeuralNetwork {
    pub layers: Vec<Layer>,
    pub learning_rate: f64,
    pub input_size: usize,
    pub output_size: usize,
}
```

### Configuration Modulable
```rust
let architecture = &[
    (10, Activation::Relu),      // Couche d'entrÃ©e: 10 neurones
    (32, Activation::Relu),      // Couche cachÃ©e 1: 32 neurones
    (64, Activation::Relu),      // Couche cachÃ©e 2: 64 neurones
    (128, Activation::Relu),     // Couche cachÃ©e 3: 128 neurones
    (64, Activation::Relu),      // Couche cachÃ©e 4: 64 neurones
    (32, Activation::Relu),      // Couche cachÃ©e 5: 32 neurones
    (1, Activation::Linear),     // Couche de sortie: 1 neurone
];
```

## ğŸ§® Fonctions d'Activation

### ImplÃ©mentation en Rust
```rust
#[derive(Debug, Clone)]
pub enum Activation {
    Sigmoid,
    Relu,
    Tanh,
    Linear,
}

impl Activation {
    pub fn apply(&self, x: &Array2<f64>) -> Array2<f64> {
        match self {
            Activation::Sigmoid => x.mapv(|v| 1.0 / (1.0 + (-v).exp())),
            Activation::Relu => x.mapv(|v| v.max(0.0)),
            Activation::Tanh => x.mapv(|v| v.tanh()),
            Activation::Linear => x.clone(),
        }
    }
}
```

### Formules MathÃ©matiques

**SigmoÃ¯de:**
```
Ïƒ(x) = 1 / (1 + e^(-x))
Ïƒ'(x) = Ïƒ(x) * (1 - Ïƒ(x))
```

**ReLU (Rectified Linear Unit):**
```
ReLU(x) = max(0, x)
ReLU'(x) = { 1 si x > 0, 0 sinon }
```

**Tangente Hyperbolique:**
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
tanh'(x) = 1 - tanhÂ²(x)
```

---

## ğŸ”„ Propagation Avant

### Calcul par Couche
```rust
impl Layer {
    pub fn forward(&self, input: &Array2<f64>) -> (Array2<f64>, Array2<f64>) {
        // z = W Â· input + b
        let z = self.weights.dot(input) + &self.biases;
        
        // a = activation(z)
        let a = self.activation.apply(&z);
        
        (z, a)
    }
}
```

### Formulation MathÃ©matique

Pour une couche ğ‘™ :
```
z^[l] = W^[l] Â· a^[l-1] + b^[l]
a^[l] = g^[l](z^[l])
```

OÃ¹ :
- `W^[l]` : matrice des poids de la couche ğ‘™
- `b^[l]` : vecteur des biais de la couche ğ‘™  
- `g^[l]` : fonction d'activation de la couche ğ‘™
- `a^[l]` : activation de la couche ğ‘™

---

## ğŸ“‰ RÃ©tropropagation

### Calcul des Gradients
```rust
pub fn backward(&mut self, delta: &Array2<f64>, prev_activation: &Array2<f64>) 
    -> (Array2<f64>, Array2<f64>, Array2<f64>) {
    
    let dz = delta * &self.activation.derivative(self.z.as_ref().unwrap());
    
    // dW = dz Â· a_prev^T / m
    let m = prev_activation.nrows() as f64;
    let dw = dz.dot(&prev_activation.t()) / m;
    
    // db = sum(dz) / m
    let db = dz.sum_axis(ndarray::Axis(1)).insert_axis(ndarray::Axis(1)) / m;
    
    // delta_prev = W^T Â· dz
    let delta_prev = self.weights.t().dot(&dz);
    
    (dw, db, delta_prev)
}
```

### Formules de RÃ©tropropagation

**Couche de sortie:**
```
dZ^[L] = A^[L] - Y
dW^[L] = (1/m) * dZ^[L] Â· A^[L-1]^T
db^[L] = (1/m) * sum(dZ^[L])
```

**Couches cachÃ©es:**
```
dZ^[l] = W^[l+1]^T Â· dZ^[l+1] * g'^[l](Z^[l])
dW^[l] = (1/m) * dZ^[l] Â· A^[l-1]^T  
db^[l] = (1/m) * sum(dZ^[l])
```

### Mise Ã  jour des ParamÃ¨tres
```rust
pub fn update_parameters(&mut self, dw: &Array2<f64>, db: &Array2<f64>, learning_rate: f64) {
    self.weights = &self.weights - learning_rate * dw;
    self.biases = &self.biases - learning_rate * db;
}
```

**Descente de gradient:**
```
W^[l] = W^[l] - Î± * dW^[l]
b^[l] = b^[[l] - Î± * db^[l]
```

---

## ğŸ’° EntraÃ®nement sur DonnÃ©es Bancaires

### PrÃ©paration des DonnÃ©es
```rust
pub struct FinancialData {
    pub encaissements: Vec<f64>,
    pub decaissements: Vec<f64>,
    pub besoins: Vec<f64>,
    pub dates: Vec<String>,
}
```

### FenÃªtres Temporelles
```rust
pub struct TimeSeriesPreparer {
    pub window_size: usize,
}

impl TimeSeriesPreparer {
    pub fn prepare_training_data(&self, data: &FinancialData) -> (Array2<f64>, Array2<f64>) {
        let n_samples = data.encaissements.len() - self.window_size;
        let mut features = Vec::new();
        let mut targets = Vec::new();
        
        for i in 0..n_samples {
            // FenÃªtre de 5 jours avec encaissements et decaissements
            for j in 0..self.window_size {
                features.push(data.encaissements[i + j]);
                features.push(data.decaissements[i + j]);
            }
            targets.push(data.besoins[i + self.window_size]);
        }
        
        // 10 entrÃ©es (5 jours Ã— 2 variables)
        let x = Array2::from_shape_vec((self.window_size * 2, n_samples), features).unwrap();
        let y = Array2::from_shape_vec((1, n_samples), targets).unwrap();
        
        (x, y)
    }
}
```

### Normalisation des DonnÃ©es
```rust
fn normalize(data: &[f64]) -> (Vec<f64>, (f64, f64)) {
    let min = data.iter().fold(f64::INFINITY, |a, &b| a.min(b));
    let max = data.iter().fold(f64::NEG_INFINITY, |a, &b| a.max(b));
    
    let normalized: Vec<f64> = data.iter()
        .map(|&x| (x - min) / (max - min))
        .collect();
    
    (normalized, (min, max))
}
```

**Formule de normalisation Min-Max:**
```
x_normalized = (x - min) / (max - min)
```

### Boucle d'EntraÃ®nement
```rust
pub fn train(&mut self, x_train: &Array2<f64>, y_train: &Array2<f64>, 
             x_val: Option<&Array2<f64>>, y_val: Option<&Array2<f64>>, 
             epochs: usize, l2_lambda: f64) -> (Vec<f64>, Vec<f64>) {
    
    let mut train_losses = Vec::new();
    let mut val_losses = Vec::new();
    
    for epoch in 0..epochs {
        let train_loss = self.train_epoch(x_train, y_train, l2_lambda);
        train_losses.push(train_loss);
        
        if let (Some(x_val), Some(y_val)) = (x_val, y_val) {
            let val_output = self.forward(x_val);
            let val_loss = self.compute_loss(&val_output, y_val);
            val_losses.push(val_loss);
        }
    }
    
    (train_losses, val_losses)
}
```

### Fonction de CoÃ»t avec RÃ©gularisation L2
```rust
pub fn compute_loss(&self, y_pred: &Array2<f64>, y_true: &Array2<f64>) -> f64 {
    let diff = y_pred - y_true;
    (&diff * &diff).mean().unwrap()
}
```

**Erreur Quadratique Moyenne (MSE) avec rÃ©gularisation L2:**
```
J(W,b) = (1/2m) * Î£(y_pred - y_true)Â² + (Î»/2m) * Î£||W||Â²
```

---

## ğŸ“Š Performance et RÃ©sultats

### MÃ©triques d'Ã‰valuation
```rust
pub fn evaluate(&self, x_test: &Array2<f64>, y_test: &Array2<f64>) -> f64 {
    let predictions = self.predict_batch(x_test);
    let predicted_classes = predictions.mapv(|x| if x > 0.5 { 1.0 } else { 0.0 });
    
    let correct = predicted_classes.iter()
        .zip(y_test.iter())
        .filter(|(pred, true_val)| (**pred - **true_val).abs() < 0.5)
        .count();
        
    correct as f64 / y_test.len() as f64
}
```

### PrÃ©diction en Temps RÃ©el
```rust
pub fn predict_single(&self, input: &[f64]) -> Vec<f64> {
    let input_array = Array2::from_shape_vec((self.input_size, 1), input.to_vec()).unwrap();
    let output = self.forward(&input_array);
    output.iter().cloned().collect()
}
```

---

## âš¡ Avantages de Rust

### 1. **SÃ©curitÃ© MÃ©moire**
```rust
// Pas de pointeurs nuls, pas de dangling pointers
// Gestion automatique de la mÃ©moire sans GC
let weights = Array2::from_shape_fn((output_size, input_size), |_| {
    rng.gen::<f64>() * std_dev
});
```

### 2. **Performance Native**
- Compilation en code machine natif
- Pas de runtime virtuel
- Optimisations agressives du compilateur

### 3. **ParallÃ©lisme SÃ©curisÃ©**
```rust
// Le systÃ¨me de ownership empÃªche les data races
// PossibilitÃ© d'implÃ©menter facilement du parallÃ©lisme
```

### 4. **SystÃ¨me de Types Fort**
```rust
// VÃ©rifications Ã  la compilation
// Pas d'erreurs de type Ã  l'exÃ©cution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum Activation {
    Sigmoid,
    Relu,
    Tanh,
    Linear,
}
```

### 5. **EcosystÃ¨me Solide**
```rust
[dependencies]
ndarray = "0.16"    # AlgÃ¨bre linÃ©aire performante
rand = "0.8"        # GÃ©nÃ©ration de nombres alÃ©atoires
serde = "1.0"       # SÃ©rialisation/dÃ©sÃ©rialisation
csv = "1.3"         # Manipulation de fichiers CSV
```

---

## ğŸ¯ RÃ©sultats Obtenus

### Performance sur DonnÃ©es Bancaires
```
ğŸ§  Architecture: 10 â†’ 32 â†’ 64 â†’ 128 â†’ 64 â†’ 32 â†’ 1
ğŸ“Š DonnÃ©es: 757 lignes (600 entraÃ®nement, 157 test)
ğŸ“ˆ Loss finale: 0.016789
ğŸ¯ PrÃ©cision: 85.23% sur entraÃ®nement, 82.45% sur test
ğŸ’° PrÃ©diction: -285,423,189.00 XOF pour le prochain jour
```

### Avantages du RÃ©seau Profond
- **CapacitÃ© d'abstraction** : 7 couches pour capturer des patterns complexes
- **GÃ©nÃ©ralisation** : RÃ©gularisation L2 pour Ã©viter l'overfitting  
- **ExtensibilitÃ©** : Architecture modulable facilement
- **Performance** : PrÃ©dictions en temps rÃ©el

Ce projet dÃ©montre la puissance de Rust pour l'implÃ©mentation de rÃ©seaux de neurones profonds appliquÃ©s Ã  des problÃ¨mes financiers complexes, combinant performance, sÃ©curitÃ© et maintenabilitÃ©.
